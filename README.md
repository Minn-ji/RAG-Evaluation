# RAG 평가 시스템 README

## 1. 소개

이 문서는 RAG (Retrieval-Augmented Generation) 모델의 성능을 평가하는 시스템에 대한 설명서입니다. 개발자가 아닌 분들도 시스템의 작동 방식과 다양한 평가 지표의 의미를 쉽게 이해하고, 평가 결과를 바탕으로 비즈니스 인사이트를 얻을 수 있도록 작성되었습니다.

본 시스템은 RAG 모델의 두뇌와 팔다리에 해당하는 **검색(Retrieval)** 능력과 **생성(Generation)** 능력을 개별적 또는 통합적으로 측정하여, 우리 AI 모델의 강점과 약점을 다각도로 분석합니다.

<br>

## 2. 핵심 컨셉: RAG (검색 증강 생성) 란 무엇인가요?

RAG는 '검색 증강 생성'의 약자입니다. AI가 질문에 답변하는 방식을 비유를 통해 쉽게 이해해 보겠습니다.

- **전통적인 AI 모델:** 모든 것을 암기해서 시험을 보는 학생과 같습니다. 외운 것 안에서는 잘 대답하지만, 암기하지 않은 내용이나 최신 정보에 대해서는 답변하지 못하거나 틀린 정보를 말할 수 있습니다.
- **RAG 모델:** **오픈북 시험**을 보는 학생과 같습니다. 질문을 받으면, 먼저 방대한 자료실(Vector DB)에서 질문과 관련된 가장 정확한 정보가 담긴 책(문서)을 **찾아옵니다(검색)**. 그리고 그 책의 내용을 바탕으로 논리 정연하게 자신의 답변을 **만들어냅니다(생성)**.

이처럼 RAG 모델은 **정확한 정보 검색 능력**과 **유창한 답변 생성 능력**이라는 두 가지 핵심 역량을 가지고 있습니다. 저희 평가 시스템은 이 두 가지 능력을 정밀하게 측정하고 분석하는 역할을 합니다.

<br>

## 3. 평가 시스템의 전체 구조 (LangGraph)

저희 시스템은 `LangGraph`라는 도구를 사용하여, 복잡한 평가 과정을 마치 잘 짜인 공장의 생산 라인처럼 자동화하고 체계적으로 관리합니다.

중심에는 **메인 그래프(Main Graph)**가 있어, 사용자가 원하는 평가 방식에 따라 작업을 지시하는 '교통정리' 역할을 합니다. 사용자는 다음 세 가지 평가 모드 중 하나를 선택할 수 있습니다.

![workflow](https://i.imgur.com/9zv0B4p.png)

#### 세 가지 평가 모드:

1.  **✅ 검색기 평가 모드 (Retrieval-Only)**
    -   **무엇을 평가하나요?** : AI가 사용자의 질문 의도에 맞는 문서를 얼마나 정확하고 빠르게 찾아오는지 '검색 능력'만을 집중적으로 평가합니다.
    -   **핵심 질문**: "AI가 정답이 담긴 문서를 추천 목록의 최상단에 보여주는가?"

2.  **✍️ 생성기 평가 모드 (Generation-Only)**
    -   **무엇을 평가하나요?** : 정답 문서를 미리 제공했을 때, AI가 이 문서를 바탕으로 얼마나 자연스럽고 정확한 답변을 만들어내는지 '생성 능력'만을 평가합니다.
    -   **핵심 질문**: "AI가 주어진 자료를 잘 요약해서 논리적인 답변을 구성하는가?"

3.  **🚀 전체 평가 모드 (Full / End-to-End)**
    -   **무엇을 평가하나요?** : 사용자가 질문을 던지는 순간부터 최종 답변을 받기까지의 전 과정을 종합적으로 평가합니다. 검색과 생성을 모두 포함하여, 실제 서비스 환경과 가장 유사한 성능을 측정합니다.
    -   **핵심 질문**: "사용자 입장에서, 질문부터 답변까지 전체 과정의 만족도가 얼마나 높은가?"

<br>

## 4. 평가 지표 상세 설명

AI 모델의 성능을 다양한 관점에서 측정하기 위해 아래와 같은 전문적인 평가 지표들을 사용합니다. 각 지표의 의미를 쉽게 풀어 설명했습니다.

### 📈 검색 (Retrieval) 성능 지표

> 검색 성능은 'AI가 얼마나 똑똑하게 정답 문서를 찾아오는가'를 측정합니다.

| 지표 (영문)             | 지표 (한글)         | 설명                                                                                                                                                           | 높을수록 좋은가? | 관련 파일                               |
| ----------------------- | ------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------: | --------------------------------------- |
| **MRR**                 | 평균 상호 순위      | AI가 추천한 여러 문서 중, '첫 번째 정답'이 몇 번째 순위에 나타나는지 측정합니다. 정답을 얼마나 빨리(높은 순위에) 보여주는지를 나타내는 지표입니다.                 |        ✅        | `metrics/Retrieval/MRR.py`              |
| **MAP**                 | 평균 정확도         | 추천된 문서 목록 전체의 정확도를 종합적으로 평가합니다. 단순히 정답을 맞췄는지를 넘어, 추천 목록 상위에 정답 문서들이 얼마나 많이 분포해 있는지를 측정합니다.       |        ✅        | `metrics/Retrieval/MAP.py`              |
| **Precision**           | 정밀도              | AI가 '정답'이라고 예측한 문서들 중, 실제 정답 문서의 비율입니다. "AI의 예측이 얼마나 정밀한가?"를 보여줍니다.                                                     |        ✅        | `metrics/Retrieval/precision.py`        |
| **Recall**              | 재현율              | 실제 정답 문서들 중에서, AI가 얼마나 많이 누락하지 않고 찾아냈는지를 나타냅니다. "AI가 얼마나 빠짐없이 잘 찾아내는가?"를 보여줍니다.                             |        ✅        | `metrics/Retrieval/recall.py`           |
| **F1 Score**            | F1 점수             | 정밀도(Precision)와 재현율(Recall)의 조화 평균입니다. 두 지표가 모두 중요할 때 사용하며, 한쪽에 치우치지 않은 종합적인 성능을 보여줍니다.                         |        ✅        | `metrics/Retrieval/RetrievalEvaluator.py` |

### ✍️ 생성 (Generation) 성능 지표

> 생성 성능은 'AI가 찾아온 정보를 바탕으로 얼마나 완성도 높은 답변을 만들어내는가'를 측정합니다.

| 지표 (영문)             | 지표 (한글)         | 설명                                                                                                                                                           | 높을수록 좋은가? | 관련 파일                                   |
| ----------------------- | ------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------: | ------------------------------------------- |
| **ROUGE**               | 루지 점수           | AI가 생성한 답변과 사람이 작성한 모범 답안 사이에 겹치는 단어나 구문이 얼마나 많은지를 측정합니다. 주로 답변에 핵심 내용이 포함되었는지를 평가합니다.               |        ✅        | `metrics/Generation/ROUGE.py`               |
| **BLEU**                | 블루 점수           | ROUGE와 유사하지만, 생성된 답변의 문법적 완성도와 유창성을 함께 고려합니다. 기계 번역 품질 평가에 처음 사용되었으며, 문장의 자연스러움을 측정하는 데 강점이 있습니다. |        ✅        | `metrics/Generation/BLEU.py`                |
| **Faithfulness**        | 충실성 / 진실성     | AI가 생성한 답변이 제공된 문서(Context)의 내용에 얼마나 충실한지를 평가합니다. AI가 정보를 왜곡하거나 지어내지 않고, 주어진 사실에 기반하여 답변했는지를 측정하는 매우 중요한 지표입니다. |        ✅        | `metrics/Generation/faithfulness.py`        |
| **BERTScore**           | 버트 스코어         | 단순히 단어의 일치 여부를 넘어, BERT라는 AI 모델을 이용해 문맥적인 의미의 유사도를 측정합니다. 같은 의미지만 다른 단어로 표현된 경우도 잘 잡아낼 수 있습니다.       |        ✅        | `metrics/Generation/BertScore.py`           |
| **LLM-as-Judge**        | AI 심판 평가        | 더 강력한 성능의 상위 AI 모델(예: GPT-4)을 심판으로 두고, 생성된 답변의 종합적인 품질(논리, 정확성, 유창성 등)을 평가하게 하는 방식입니다. 정량화하기 어려운 미묘한 품질 차이를 잡아낼 수 있습니다. |        ✅        | `metrics/Generation/LLM_as_Judge.py`        |

<br>

## 5. 결론

본 RAG 평가 시스템을 통해, 저희는 AI 모델의 성능을 객관적이고 다각적으로 분석할 수 있습니다. 이 평가 결과를 바탕으로 모델의 약점을 보완하고 강점을 극대화하여, 사용자에게 더욱 신뢰도 높고 유용한 정보를 제공하는 AI 서비스를 만들어나갈 것입니다.