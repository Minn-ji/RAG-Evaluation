{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d216f229",
   "metadata": {},
   "source": [
    "# Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac391ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"/Users/claion/Projects/experimental-code/rag-bench-marking-test/data/new_rag-eval-ko-dataset-public.csv\"\n",
    "dataset = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e22b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1ea6d5",
   "metadata": {},
   "source": [
    "# Connect VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a99148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "result_base_path = (\n",
    "    \"/Users/claion/Projects/experimental-code/rag-bench-marking-test/data/results\"\n",
    ")\n",
    "os.makedirs(result_base_path, exist_ok=True)\n",
    "result_file_name = \"03-04_bmt_result.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16c85ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "HOST = \"localhost\"\n",
    "PORT = 6333\n",
    "COLLECTION_NAME = \"bench-marking-test\"\n",
    "\n",
    "DENSE_VECTOR = \"dense\"\n",
    "SPARSE_VECTOR = \"sparse\"\n",
    "\n",
    "client = QdrantClient(host=HOST, port=PORT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa6d000",
   "metadata": {},
   "source": [
    "# Load Embedding Model and Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701558e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding import DenseEmbedding, SparseEmbedding\n",
    "\n",
    "dense_model = DenseEmbedding()\n",
    "sparse_model = SparseEmbedding()\n",
    "\n",
    "test_query = \"model loading\"\n",
    "\n",
    "dense_model.query_embed(test_query)\n",
    "sparse_model.query_embed(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb348ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "retrieve_result = {\n",
    "    \"question\": [],\n",
    "    \"target_file_name\": [],\n",
    "    \"target_page_no\": [],\n",
    "    \"latency_secs\": [],\n",
    "    \"retrieved_doc1\": [],\n",
    "    \"retrieved_page1\": [],\n",
    "    \"retrieved_cont1\": [],\n",
    "    \"retrieved_doc2\": [],\n",
    "    \"retrieved_page2\": [],\n",
    "    \"retrieved_cont2\": [],\n",
    "    \"retrieved_doc3\": [],\n",
    "    \"retrieved_page3\": [],\n",
    "    \"retrieved_cont3\": [],\n",
    "    \"retrieved_doc4\": [],\n",
    "    \"retrieved_page4\": [],\n",
    "    \"retrieved_cont4\": [],\n",
    "    \"retrieved_doc5\": [],\n",
    "    \"retrieved_page5\": [],\n",
    "    \"retrieved_cont5\": [],\n",
    "}\n",
    "\n",
    "for idx, query in enumerate(dataset.question):\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # VectorDB 검색\n",
    "    q_dense_embedding = dense_model.query_embed(query)\n",
    "    q_sparse_embedding = sparse_model.query_embed(query)\n",
    "\n",
    "    pref_limit = 20\n",
    "    prefetch = [\n",
    "        models.Prefetch(\n",
    "            query=q_dense_embedding,\n",
    "            using=DENSE_VECTOR,\n",
    "            limit=pref_limit,\n",
    "        ),\n",
    "        models.Prefetch(\n",
    "            query=models.SparseVector(**q_sparse_embedding),\n",
    "            using=SPARSE_VECTOR,\n",
    "            limit=pref_limit,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    results = client.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        prefetch=prefetch,\n",
    "        query=models.FusionQuery(fusion=models.Fusion.DBSF),\n",
    "        limit=5,\n",
    "    ).model_dump()\n",
    "\n",
    "    retrieve_result[\"question\"].append(query)\n",
    "    retrieve_result[\"target_file_name\"].append(dataset.target_file_name[idx])\n",
    "    retrieve_result[\"target_page_no\"].append(dataset.target_page_no[idx])\n",
    "\n",
    "    # 검색결과저장\n",
    "    for p_idx, point in enumerate(results[\"points\"]):\n",
    "        _payload: dict = point[\"payload\"]\n",
    "        # file_name = _payload[\"file_name\"]\n",
    "        # page = _payload[\"page\"]\n",
    "        # content = _payload[\"content\"]\n",
    "        retrieve_result[f\"retrieved_doc{p_idx + 1}\"].append(_payload[\"file_name\"])\n",
    "        retrieve_result[f\"retrieved_page{p_idx + 1}\"].append(_payload[\"page\"])\n",
    "        retrieve_result[f\"retrieved_cont{p_idx + 1}\"].append(_payload[\"content\"])\n",
    "    end_time = datetime.now()\n",
    "    latency_secs = float(f\"{(end_time - start_time).total_seconds():.4f}\")\n",
    "    retrieve_result[\"latency_secs\"].append(latency_secs)\n",
    "    print(f\">> Complete Question {idx + 1:02} / Latency: {latency_secs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d409386e",
   "metadata": {},
   "source": [
    "# Retrieve result save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06592a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(retrieve_result)\n",
    "result_df.to_csv(os.path.join(result_base_path, result_file_name), index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8d3a60",
   "metadata": {},
   "source": [
    "# Evaluate Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0379fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "result_df = pd.read_csv(os.path.join(result_base_path, result_file_name))\n",
    "\n",
    "# Evaluation\n",
    "limit = 5\n",
    "\n",
    "recalls = []\n",
    "reciprocal_ranks = []\n",
    "\n",
    "for idx in range(result_df.shape[0]):\n",
    "    target_file_name = result_df.target_file_name[idx]\n",
    "    target_page_no = result_df.target_page_no[idx]\n",
    "\n",
    "    for p_idx in range(limit):\n",
    "        rank = p_idx + 1\n",
    "        if (\n",
    "            result_df[f\"retrieved_doc{rank}\"][idx] == target_file_name\n",
    "            and result_df[f\"retrieved_page{rank}\"][idx] == target_page_no\n",
    "        ):\n",
    "            reciprocal_ranks.append(float(f\"{(1 / rank):.2f}\"))\n",
    "            recalls.append(1)\n",
    "            break\n",
    "    else:\n",
    "        reciprocal_ranks.append(0)\n",
    "        recalls.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d5bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 결과 추가\n",
    "result_df[\"recall\"] = recalls\n",
    "result_df[\"reciprocal_rank\"] = reciprocal_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eb7fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 결과를 포함해 다시 저장\n",
    "result_df.to_csv(os.path.join(result_base_path, result_file_name), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f9c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종평가결과\n",
    "import math\n",
    "\n",
    "\n",
    "def get_cnt_by_ratio(total_len: int, ratio: float = 0.95):\n",
    "    return math.ceil(total_len * ratio)\n",
    "\n",
    "\n",
    "r_95 = get_cnt_by_ratio(len(result_df.latency_secs), 0.95)\n",
    "\n",
    "recall_5 = sum(recalls) / len(recalls)\n",
    "mrr_5 = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "latency_95 = sum(sorted(result_df.latency_secs)[:r_95]) / r_95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RRF_10\n",
    "print(f\"{recall_5=}\")\n",
    "print(f\"{mrr_5=:.2f}\")\n",
    "print(f\"{latency_95=:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434fa810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RRF_20\n",
    "print(f\"{recall_5=}\")\n",
    "print(f\"{mrr_5=:.2f}\")\n",
    "print(f\"{latency_95=:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0146d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSF_10\n",
    "print(f\"{recall_5=}\")\n",
    "print(f\"{mrr_5=:.2f}\")\n",
    "print(f\"{latency_95=:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd55348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSF_20\n",
    "print(f\"{recall_5=}\")\n",
    "print(f\"{mrr_5=:.2f}\")\n",
    "print(f\"{latency_95=:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-bench-marking-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
